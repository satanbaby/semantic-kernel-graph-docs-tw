# æ–‡ä»¶åˆ†æç®¡é“ç¯„ä¾‹

æ­¤ç¯„ä¾‹å±•ç¤ºä½¿ç”¨èªç¾©æ ¸å¿ƒåœ–è¡¨çš„ç¶œåˆæ–‡ä»¶è™•ç†å·¥ä½œæµç¨‹ã€‚å®ƒå±•ç¤ºå¦‚ä½•ä½¿ç”¨ä¸¦è¡Œè™•ç†å’ŒéŒ¯èª¤è™•ç†æ§‹å»ºå¤šéšæ®µæ–‡ä»¶æ”å…¥ã€åˆ†æã€åˆ†é¡å’Œè³‡è¨Šæ“·å–ç®¡é“ã€‚

## ç›®æ¨™

äº†è§£å¦‚ä½•åœ¨åŸºæ–¼åœ–è¡¨çš„å·¥ä½œæµç¨‹ä¸­å¯¦ç¾æ–‡ä»¶åˆ†æç®¡é“ï¼Œä»¥ï¼š
* é€šéå¤šå€‹åˆ†æéšæ®µè™•ç†æ–‡ä»¶
* å¯¦ç¾ä¸¦è¡Œè™•ç†ä»¥å„ªåŒ–æ•ˆèƒ½
* è™•ç†ä¸åŒçš„æ–‡ä»¶é¡å‹å’Œæ ¼å¼
* å¾éçµæ§‹åŒ–æ–‡æœ¬æå–çµæ§‹åŒ–è³‡è¨Š
* ä½¿ç”¨éŒ¯èª¤è™•ç†å’Œæ¢å¾©æ§‹å»ºå½ˆæ€§ç®¡é“
* è·¨å¤šå€‹å·¥ä½œç¨‹åºæ“´å±•æ–‡ä»¶è™•ç†

## å‰ææ¢ä»¶

* **.NET 8.0** æˆ–æ›´é«˜ç‰ˆæœ¬
* **OpenAI API é‡‘é‘°**é…ç½®åœ¨ `appsettings.json` ä¸­
* **èªç¾©æ ¸å¿ƒåœ–è¡¨å¥—ä»¶**å·²å®‰è£
* å°[åœ–è¡¨æ¦‚å¿µ](../concepts/graph-concepts.md)å’Œ[ç¯€é»é¡å‹](../concepts/node-types.md)çš„åŸºæœ¬äº†è§£

## ä¸»è¦å…ƒä»¶

### æ¦‚å¿µå’ŒæŠ€è¡“

* **ç®¡é“è™•ç†**ï¼šæ–‡ä»¶åˆ†æéšæ®µçš„é †åºåŸ·è¡Œ
* **ä¸¦è¡Œè™•ç†**ï¼šç¨ç«‹åˆ†æä»»å‹™çš„ä¸¦è¡ŒåŸ·è¡Œ
* **æ–‡ä»¶åˆ†é¡**ï¼šæŒ‰é¡å‹å’Œå…§å®¹è‡ªå‹•åˆ†é¡æ–‡ä»¶
* **è³‡è¨Šæ“·å–**ï¼šå¾éçµæ§‹åŒ–æ–‡æœ¬æå–çµæ§‹åŒ–è³‡æ–™
* **éŒ¯èª¤è™•ç†**ï¼šå„ªé›…è™•ç†è™•ç†å¤±æ•—å’Œæ¢å¾©

### æ ¸å¿ƒé¡åˆ¥

* `FunctionGraphNode`ï¼šåŸ·è¡Œæ–‡ä»¶è™•ç†åŠŸèƒ½
* `ConditionalGraphNode`ï¼šåŸºæ–¼é¡å‹å’Œå…§å®¹è·¯ç”±æ–‡ä»¶
* `GraphExecutor`ï¼šå”èª¿æ–‡ä»¶åˆ†æç®¡é“
* `GraphState`ï¼šåœ¨æ•´å€‹è™•ç†éç¨‹ä¸­ç®¡ç†æ–‡ä»¶ç‹€æ…‹å’Œå…ƒè³‡æ–™

## åŸ·è¡Œç¯„ä¾‹

### å…¥é–€

æ­¤ç¯„ä¾‹å±•ç¤ºä½¿ç”¨èªç¾©æ ¸å¿ƒåœ–è¡¨å¥—ä»¶çš„æ–‡ä»¶åˆ†æç®¡é“å·¥ä½œæµç¨‹ã€‚ä»¥ä¸‹ç¨‹å¼ç¢¼ç‰‡æ®µå±•ç¤ºå¦‚ä½•åœ¨è‡ªå·±çš„æ‡‰ç”¨ç¨‹å¼ä¸­å¯¦ç¾æ­¤æ¨¡å¼ã€‚

## é€æ­¥å¯¦ç¾

### 1. åŸºæœ¬æ–‡ä»¶åˆ†æç®¡é“

æ­¤ç¯„ä¾‹å±•ç¤ºä¸€å€‹ç°¡å–®çš„ä¸‰éšæ®µæ–‡ä»¶è™•ç†å·¥ä½œæµç¨‹ã€‚

```csharp
// ä½¿ç”¨æ¨¡æ“¬é…ç½®å»ºç«‹æ ¸å¿ƒ
var kernel = CreateKernel();

// å»ºç«‹æ–‡ä»¶åˆ†æç®¡é“
var pipeline = new GraphExecutor("DocumentAnalysisPipeline", "Basic document analysis", logger);

// éšæ®µ 1ï¼šæ–‡ä»¶æ”å…¥
var documentIngestion = new FunctionGraphNode(
    "document-ingestion",
    "Ingest and validate document",
    async (context) =>
    {
        var documentPath = context.GetValue<string>("document_path");
        var documentContent = await File.ReadAllTextAsync(documentPath);
        
        // æå–åŸºæœ¬å…ƒè³‡æ–™
        var fileInfo = new FileInfo(documentPath);
        var metadata = new Dictionary<string, object>
        {
            ["file_name"] = fileInfo.Name,
            ["file_size"] = fileInfo.Length,
            ["file_extension"] = fileInfo.Extension,
            ["ingestion_timestamp"] = DateTime.UtcNow,
            ["content_length"] = documentContent.Length
        };

        // åœ¨åŸ·è¡Œå…§å®¹ä¸Šå„²å­˜å…§å®¹å’Œå…ƒè³‡æ–™ï¼Œä¾›ä¸‹æ¸¸ç¯€é»ä½¿ç”¨
        context.SetValue("document_content", documentContent);
        context.SetValue("document_metadata", metadata);
        // åŒæ™‚ç›´æ¥å…¬é–‹æ–‡ä»¶å‰¯æª”åä»¥ç°¡åŒ–ä¸‹æ¸¸å­˜å–
        context.SetValue("file_extension", fileInfo.Extension);
        context.SetValue("processing_status", "ingested");
        
        return $"Document ingested: {fileInfo.Name} ({fileInfo.Length} bytes)";
    });

// éšæ®µ 2ï¼šæ–‡ä»¶åˆ†é¡
var documentClassification = new FunctionGraphNode(
    "document-classification",
    "Classify document by type and content",
    async (context) =>
    {
        var content = context.GetValue<string>("document_content");
        var extension = context.GetValue<string>("file_extension");
        
        // ç°¡å–®çš„åˆ†é¡é‚è¼¯
        var documentType = extension.ToLower() switch
        {
            ".txt" => "text",
            ".md" => "markdown",
            ".pdf" => "pdf",
            ".doc" => "word",
            ".docx" => "word",
            _ => "unknown"
        };
        
        // åŸºæ–¼å…§å®¹çš„åˆ†é¡
        var contentCategory = content.ToLower() switch
        {
            var c when c.Contains("invoice") || c.Contains("bill") => "financial",
            var c when c.Contains("contract") || c.Contains("agreement") => "legal",
            var c when c.Contains("report") || c.Contains("analysis") => "report",
            var c when c.Contains("email") || c.Contains("correspondence") => "communication",
            _ => "general"
        };
        
        // å°‡åˆ†é¡çµæœä¿ç•™åœ¨å…§å®¹ä¸­
        context.SetValue("document_type", documentType);
        context.SetValue("content_category", contentCategory);
        context.SetValue("processing_status", "classified");

        return $"Document classified as {documentType} ({contentCategory})";
    });

// éšæ®µ 3ï¼šå…§å®¹åˆ†æ
var contentAnalysis = new FunctionGraphNode(
    "content-analysis",
    "Analyze document content and extract key information",
    async (context) =>
    {
        var content = context.GetValue<string>("document_content");
        var documentType = context.GetValue<string>("document_type");
        var contentCategory = context.GetValue<string>("content_category");
        
        // æ ¹æ“šæ–‡ä»¶é¡å‹æå–é—œéµè³‡è¨Š
        var analysis = new Dictionary<string, object>();
        
        switch (contentCategory)
        {
            case "financial":
                analysis["key_amounts"] = ExtractAmounts(content);
                analysis["dates"] = ExtractDates(content);
                analysis["parties"] = ExtractParties(content);
                break;
            case "legal":
                analysis["contract_parties"] = ExtractParties(content);
                analysis["effective_dates"] = ExtractDates(content);
                analysis["key_terms"] = ExtractKeyTerms(content);
                break;
            case "report":
                analysis["summary"] = GenerateSummary(content);
                analysis["key_findings"] = ExtractFindings(content);
                analysis["recommendations"] = ExtractRecommendations(content);
                break;
            default:
                analysis["word_count"] = content.Split(' ').Length;
                analysis["paragraph_count"] = content.Split('\n').Length;
                analysis["key_topics"] = ExtractTopics(content);
                break;
        }
        
        // å„²å­˜åˆ†æçµæœä¸¦æ›´æ–°è™•ç†ç‹€æ…‹
        context.SetValue("content_analysis", analysis);
        context.SetValue("processing_status", "analyzed");
        context.SetValue("analysis_timestamp", DateTime.UtcNow);
        
        return $"Content analysis completed for {documentType} document";
    });

// å°‡ç¯€é»æ–°å¢åˆ°ç®¡é“
pipeline.AddNode(documentIngestion);
pipeline.AddNode(documentClassification);
pipeline.AddNode(contentAnalysis);

// è¨­å®šé–‹å§‹ç¯€é»
pipeline.SetStartNode(documentIngestion.NodeId);

// ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶æ¸¬è©¦
var testDocuments = new[]
{
    "sample_invoice.txt",
    "contract_agreement.md",
    "quarterly_report.txt"
};

foreach (var docPath in testDocuments)
{
    var arguments = new KernelArguments
    {
        ["document_path"] = docPath
    };

    Console.WriteLine($"ğŸ“„ Processing document: {docPath}");
    var result = await pipeline.ExecuteAsync(kernel, arguments);
    
    var status = result.GetValue<string>("processing_status");
    var docType = result.GetValue<string>("document_type");
    var category = result.GetValue<string>("content_category");
    
    Console.WriteLine($"   Status: {status}");
    Console.WriteLine($"   Type: {docType}");
    Console.WriteLine($"   Category: {category}");
    
    if (result.TryGetValue("content_analysis", out var analysis))
    {
        Console.WriteLine($"   Analysis: {analysis}");
    }
    Console.WriteLine();
}
```

### 2. å…·æœ‰ä¸¦è¡Œè™•ç†çš„é€²éšç®¡é“

å±•ç¤ºç¨ç«‹åˆ†æä»»å‹™çš„ä¸¦è¡ŒåŸ·è¡Œä»¥æ”¹é€²æ•ˆèƒ½ã€‚

```csharp
// ä½¿ç”¨ä¸¦è¡Œè™•ç†å»ºç«‹é€²éšç®¡é“
var advancedPipeline = new GraphExecutor("AdvancedDocumentPipeline", "Parallel document analysis", logger);

// æ–‡ä»¶æ”å…¥ï¼ˆé †åºï¼‰
var advancedIngestion = new FunctionGraphNode(
    "advanced-ingestion",
    "Advanced document ingestion with validation",
    async (context) =>
    {
        var documentPath = context.GetValue<string>("document_path");
        var documentContent = await File.ReadAllTextAsync(documentPath);
        
        // é©—è­‰æ–‡ä»¶
        if (string.IsNullOrWhiteSpace(documentContent))
        {
            throw new InvalidOperationException("Document content is empty");
        }
        
        // æå–å…¨é¢çš„å…ƒè³‡æ–™
        var fileInfo = new FileInfo(documentPath);
        var metadata = new Dictionary<string, object>
        {
            ["file_name"] = fileInfo.Name,
            ["file_size"] = fileInfo.Length,
            ["file_extension"] = fileInfo.Extension,
            ["ingestion_timestamp"] = DateTime.UtcNow,
            ["content_length"] = documentContent.Length,
            ["line_count"] = documentContent.Split('\n').Length,
            ["word_count"] = documentContent.Split(' ').Length,
            ["character_count"] = documentContent.Length
        };
        
        context.SetValue("document_content", documentContent);
        context.SetValue("document_metadata", metadata);
        context.SetValue("processing_status", "ingested");
        
        return $"Advanced ingestion completed: {fileInfo.Name}";
    });

// ä¸¦è¡Œåˆ†æä»»å‹™
var textAnalysis = new FunctionGraphNode(
    "text-analysis",
    "Analyze text content and structure",
    async (context) =>
    {
        var content = context.GetValue<string>("document_content");
        
        // æ–‡æœ¬åˆ†æ
        var analysis = new Dictionary<string, object>
        {
            ["readability_score"] = CalculateReadabilityScore(content),
            ["sentiment_score"] = AnalyzeSentiment(content),
            ["language_detected"] = DetectLanguage(content),
            ["key_phrases"] = ExtractKeyPhrases(content),
            ["named_entities"] = ExtractNamedEntities(content)
        };
        
        context.SetValue("text_analysis", analysis);
        return "Text analysis completed";
    });

var structureAnalysis = new FunctionGraphNode(
    "structure-analysis",
    "Analyze document structure and formatting",
    async (context) =>
    {
        var content = context.GetValue<string>("document_content");
        
        // çµæ§‹åˆ†æ
        var structure = new Dictionary<string, object>
        {
            ["sections"] = IdentifySections(content),
            ["headers"] = ExtractHeaders(content),
            ["lists"] = CountLists(content),
            ["tables"] = CountTables(content),
            ["formatting"] = AnalyzeFormatting(content)
        };
        
        context.SetValue("structure_analysis", structure);
        return "Structure analysis completed";
    });

var semanticAnalysis = new FunctionGraphNode(
    "semantic-analysis",
    "Perform semantic analysis of content",
    async (context) =>
    {
        var content = context.GetValue<string>("document_content");
        
        // èªæ„åˆ†æ
        var semantic = new Dictionary<string, object>
        {
            ["topics"] = ExtractTopics(content),
            ["themes"] = IdentifyThemes(content),
            ["relationships"] = FindRelationships(content),
            ["summary"] = GenerateSemanticSummary(content),
            ["key_insights"] = ExtractKeyInsights(content)
        };
        
        context.SetValue("semantic_analysis", semantic);
        return "Semantic analysis completed";
    });

// çµæœèšåˆ
var resultsAggregation = new FunctionGraphNode(
    "results-aggregation",
    "Aggregate all analysis results",
    async (context) =>
    {
        var textAnalysis = context.GetValue<Dictionary<string, object>>("text_analysis");
        var structureAnalysis = context.GetValue<Dictionary<string, object>>("structure_analysis");
        var semanticAnalysis = context.GetValue<Dictionary<string, object>>("semantic_analysis");
        var metadata = context.GetValue<Dictionary<string, object>>("document_metadata");
        
        // çµ„åˆæ‰€æœ‰çµæœ
        var comprehensiveAnalysis = new Dictionary<string, object>
        {
            ["metadata"] = metadata,
            ["text_analysis"] = textAnalysis,
            ["structure_analysis"] = structureAnalysis,
            ["semantic_analysis"] = semanticAnalysis,
            ["analysis_timestamp"] = DateTime.UtcNow,
            ["processing_status"] = "completed"
        };
        
        context.SetValue("comprehensive_analysis", comprehensiveAnalysis);
        
        return "Results aggregation completed";
    });

// å°‡ç¯€é»æ–°å¢åˆ°ç®¡é“
advancedPipeline.AddNode(advancedIngestion);
advancedPipeline.AddNode(textAnalysis);
advancedPipeline.AddNode(structureAnalysis);
advancedPipeline.AddNode(semanticAnalysis);
advancedPipeline.AddNode(resultsAggregation);

// è¨­å®šé–‹å§‹ç¯€é»
advancedPipeline.SetStartNode(advancedIngestion.NodeId);

// æ¸¬è©¦é€²éšç®¡é“
var advancedArgs = new KernelArguments
{
    ["document_path"] = "complex_document.txt"
};

Console.WriteLine("ğŸš€ Starting advanced document analysis pipeline...");
var advancedResult = await advancedPipeline.ExecuteAsync(kernel, advancedArgs);

var comprehensiveAnalysis = advancedResult.GetValue<Dictionary<string, object>>("comprehensive_analysis");
Console.WriteLine($"âœ… Advanced analysis completed");
Console.WriteLine($"   Text Analysis: {comprehensiveAnalysis["text_analysis"]}");
Console.WriteLine($"   Structure Analysis: {comprehensiveAnalysis["structure_analysis"]}");
Console.WriteLine($"   Semantic Analysis: {comprehensiveAnalysis["semantic_analysis"]}");
```

### 3. éŒ¯èª¤è™•ç†å’Œæ¢å¾©ç®¡é“

å±•ç¤ºå¦‚ä½•ä½¿ç”¨éŒ¯èª¤è™•ç†å’Œæ¢å¾©æ©Ÿåˆ¶å¯¦ç¾å½ˆæ€§æ–‡ä»¶è™•ç†ã€‚

```csharp
// ä½¿ç”¨éŒ¯èª¤è™•ç†å»ºç«‹å½ˆæ€§ç®¡é“
var resilientPipeline = new GraphExecutor("ResilientDocumentPipeline", "Error handling and recovery", logger);

// å…·æœ‰é©—è­‰çš„æ–‡ä»¶æ”å…¥
var resilientIngestion = new FunctionGraphNode(
    "resilient-ingestion",
    "Resilient document ingestion",
    async (context) =>
    {
        try
        {
            var documentPath = context.GetValue<string>("document_path");
            
            if (!File.Exists(documentPath))
            {
                context.SetValue("error_type", "file_not_found");
                context.SetValue("error_message", $"Document not found: {documentPath}");
                context.SetValue("processing_status", "failed");
                return "Document not found";
            }
            
            var documentContent = await File.ReadAllTextAsync(documentPath);
            
            if (string.IsNullOrWhiteSpace(documentContent))
            {
                context.SetValue("error_type", "empty_content");
                context.SetValue("error_message", "Document content is empty");
                context.SetValue("processing_status", "failed");
                return "Empty document content";
            }
            
            // æˆåŠŸè·¯å¾‘
            context.SetValue("document_content", documentContent);
            context.SetValue("processing_status", "ingested");
            context.SetValue("error_type", "none");
            
            return "Document ingested successfully";
        }
        catch (Exception ex)
        {
            context.SetValue("error_type", "ingestion_error");
            context.SetValue("error_message", ex.Message);
            context.SetValue("processing_status", "failed");
            return $"Ingestion failed: {ex.Message}";
        }
    });

// åŸºæ–¼æ”å…¥ç‹€æ…‹çš„æ¢ä»¶è·¯ç”±
var ingestionRouter = new ConditionalGraphNode(
    "ingestion-router",
    "Route based on ingestion status",
    logger)
{
    ConditionExpression = "processing_status == 'ingested'",
    TrueNodeId = "content-processor",
    FalseNodeId = "error-handler"
};

// ç”¨æ–¼æˆåŠŸæ”å…¥çš„å…§å®¹è™•ç†ç¨‹å¼
var contentProcessor = new FunctionGraphNode(
    "content-processor",
    "Process document content",
    async (context) =>
    {
        try
        {
            var content = context.GetValue<string>("document_content");
            
            // è™•ç†å…§å®¹
            var processedContent = await ProcessContentAsync(content);
            context.SetValue("processed_content", processedContent);
            context.SetValue("processing_status", "processed");
            
            return "Content processing completed";
        }
        catch (Exception ex)
        {
            context.SetValue("error_type", "processing_error");
            context.SetValue("error_message", ex.Message);
            context.SetValue("processing_status", "failed");
            return $"Content processing failed: {ex.Message}";
        }
    });

// ç”¨æ–¼å¤±æ•—æ“ä½œçš„éŒ¯èª¤è™•ç†ç¨‹å¼
var errorHandler = new FunctionGraphNode(
    "error-handler",
    "Handle processing errors",
    async (context) =>
    {
        var errorType = context.GetValue<string>("error_type");
        var errorMessage = context.GetValue<string>("error_message");
        
        // è¨˜éŒ„éŒ¯èª¤
        Console.WriteLine($"âŒ Error in document processing: {errorType} - {errorMessage}");
        
        // æ ¹æ“šéŒ¯èª¤é¡å‹å˜—è©¦æ¢å¾©
        var recoveryAction = errorType switch
        {
            "file_not_found" => "Request document resubmission",
            "empty_content" => "Skip processing and notify user",
            "ingestion_error" => "Retry with exponential backoff",
            "processing_error" => "Fall back to basic processing",
            _ => "Unknown error - manual intervention required"
        };
        
        context.SetValue("recovery_action", recoveryAction);
        context.SetValue("processing_status", "error_handled");
        
        return $"Error handled. Recovery action: {recoveryAction}";
    });

// å°‡ç¯€é»æ–°å¢åˆ°å½ˆæ€§ç®¡é“
resilientPipeline.AddNode(resilientIngestion);
resilientPipeline.AddNode(ingestionRouter);
resilientPipeline.AddNode(contentProcessor);
resilientPipeline.AddNode(errorHandler);

// è¨­å®šé–‹å§‹ç¯€é»
resilientPipeline.SetStartNode(resilientIngestion.NodeId);

// æ¸¬è©¦éŒ¯èª¤è™•ç†æƒ…æ³
var errorTestScenarios = new[]
{
    new { Path = "nonexistent_file.txt", ExpectedError = "file_not_found" },
    new { Path = "empty_file.txt", ExpectedError = "empty_content" },
    new { Path = "valid_document.txt", ExpectedError = "none" }
};

foreach (var scenario in errorTestScenarios)
{
    var resilientArgs = new KernelArguments
    {
        ["document_path"] = scenario.Path
    };

    Console.WriteLine($"\nğŸ§ª Testing error handling: {scenario.Path}");
    var resilientResult = await resilientPipeline.ExecuteAsync(kernel, resilientArgs);
    
    var status = resilientResult.GetValue<string>("processing_status");
    var errorType = resilientResult.GetValue<string>("error_type");
    
    Console.WriteLine($"   Status: {status}");
    Console.WriteLine($"   Error Type: {errorType}");
    
    if (status == "error_handled")
    {
        var recoveryAction = resilientResult.GetValue<string>("recovery_action");
        Console.WriteLine($"   Recovery Action: {recoveryAction}");
    }
}
```

### 4. å¤šæ–‡ä»¶æ‰¹æ¬¡è™•ç†

å±•ç¤ºä½¿ç”¨çµæœèšåˆçš„å¤šå€‹æ–‡ä»¶ä¸¦è¡Œè™•ç†ã€‚

```csharp
// å»ºç«‹æ‰¹æ¬¡è™•ç†ç®¡é“
var batchPipeline = new GraphExecutor("BatchDocumentPipeline", "Multi-document batch processing", logger);

// æ–‡ä»¶æ‰¹æ¬¡è™•ç†ç¨‹å¼
var batchProcessor = new FunctionGraphNode(
    "batch-processor",
    "Process multiple documents in batch",
    async (context) =>
    {
        var documentPaths = context.GetValue<string[]>("document_paths");
        var batchResults = new List<Dictionary<string, object>>();
        
        // ä¸¦è¡Œè™•ç†æ–‡ä»¶
        var processingTasks = documentPaths.Select(async (docPath, index) =>
        {
            try
            {
                var content = await File.ReadAllTextAsync(docPath);
                var fileInfo = new FileInfo(docPath);
                
                var result = new Dictionary<string, object>
                {
                    ["document_id"] = $"doc_{index}",
                    ["file_name"] = fileInfo.Name,
                    ["file_size"] = fileInfo.Length,
                    ["content_length"] = content.Length,
                    ["processing_status"] = "processed",
                    ["processing_timestamp"] = DateTime.UtcNow
                };
                
                // åŸºæœ¬åˆ†æ
                result["word_count"] = content.Split(' ').Length;
                result["line_count"] = content.Split('\n').Length;
                result["key_topics"] = ExtractTopics(content);
                
                return result;
            }
            catch (Exception ex)
            {
                return new Dictionary<string, object>
                {
                    ["document_id"] = $"doc_{index}",
                    ["file_name"] = docPath,
                    ["processing_status"] = "failed",
                    ["error_message"] = ex.Message,
                    ["processing_timestamp"] = DateTime.UtcNow
                };
            }
        });
        
        var results = await Task.WhenAll(processingTasks);
        batchResults.AddRange(results);
        
        context.SetValue("batch_results", batchResults);
        context.SetValue("total_documents", documentPaths.Length);
        context.SetValue("successful_documents", batchResults.Count(r => r["processing_status"].ToString() == "processed"));
        context.SetValue("failed_documents", batchResults.Count(r => r["processing_status"].ToString() == "failed"));
        
        return $"Batch processing completed: {batchResults.Count} documents processed";
    });

// æ‰¹æ¬¡çµæœåˆ†æå™¨
var batchAnalyzer = new FunctionGraphNode(
    "batch-analyzer",
    "Analyze batch processing results",
    async (context) =>
    {
        var batchResults = context.GetValue<List<Dictionary<string, object>>>("batch_results");
        var totalDocuments = context.GetValue<int>("total_documents");
        var successfulDocuments = context.GetValue<int>("successful_documents");
        var failedDocuments = context.GetValue<int>("failed_documents");
        
        // è¨ˆç®—çµ±è¨ˆè³‡æ–™
        var totalSize = batchResults
            .Where(r => r["processing_status"].ToString() == "processed")
            .Sum(r => Convert.ToInt64(r["file_size"]));
        
        var totalWords = batchResults
            .Where(r => r["processing_status"].ToString() == "processed")
            .Sum(r => Convert.ToInt32(r["word_count"]));
        
        var analysis = new Dictionary<string, object>
        {
            ["total_documents"] = totalDocuments,
            ["successful_documents"] = successfulDocuments,
            ["failed_documents"] = failedDocuments,
            ["success_rate"] = (double)successfulDocuments / totalDocuments,
            ["total_size_bytes"] = totalSize,
            ["total_words"] = totalWords,
            ["average_file_size"] = totalSize / Math.Max(successfulDocuments, 1),
            ["average_words_per_document"] = totalWords / Math.Max(successfulDocuments, 1),
            ["processing_summary"] = batchResults
        };
        
        context.SetValue("batch_analysis", analysis);
        context.SetValue("processing_status", "batch_completed");
        
        return "Batch analysis completed";
    });

// å°‡ç¯€é»æ–°å¢åˆ°æ‰¹æ¬¡ç®¡é“
batchPipeline.AddNode(batchProcessor);
batchPipeline.AddNode(batchAnalyzer);

// è¨­å®šé–‹å§‹ç¯€é»
batchPipeline.SetStartNode(batchProcessor.NodeId);

// æ¸¬è©¦æ‰¹æ¬¡è™•ç†
var batchArgs = new KernelArguments
{
    ["document_paths"] = new[]
    {
        "document1.txt",
        "document2.md",
        "document3.txt",
        "document4.pdf"
    }
};

Console.WriteLine("ğŸ“š Starting batch document processing...");
var batchResult = await batchPipeline.ExecuteAsync(kernel, batchArgs);

var batchAnalysis = batchResult.GetValue<Dictionary<string, object>>("batch_analysis");
Console.WriteLine($"âœ… Batch processing completed");
Console.WriteLine($"   Total Documents: {batchAnalysis["total_documents"]}");
Console.WriteLine($"   Successful: {batchAnalysis["successful_documents"]}");
Console.WriteLine($"   Failed: {batchAnalysis["failed_documents"]}");
Console.WriteLine($"   Success Rate: {Convert.ToDouble(batchAnalysis["success_rate"]):P1}");
Console.WriteLine($"   Total Size: {Convert.ToInt64(batchAnalysis["total_size_bytes"]):N0} bytes");
Console.WriteLine($"   Total Words: {Convert.ToInt32(batchAnalysis["total_words"]):N0}");
```

## é æœŸè¼¸å‡º

### åŸºæœ¬æ–‡ä»¶åˆ†æç®¡é“ç¯„ä¾‹

```
ğŸ“„ Processing document: sample_invoice.txt
   Status: analyzed
   Type: text
   Category: financial
   Analysis: [key_amounts, dates, parties]

ğŸ“„ Processing document: contract_agreement.md
   Status: analyzed
   Type: markdown
   Category: legal
   Analysis: [contract_parties, effective_dates, key_terms]

ğŸ“„ Processing document: quarterly_report.txt
   Status: analyzed
   Type: text
   Category: report
   Analysis: [summary, key_findings, recommendations]
```

### å…·æœ‰ä¸¦è¡Œè™•ç†çš„é€²éšç®¡é“ç¯„ä¾‹

```
ğŸš€ Starting advanced document analysis pipeline...
âœ… Advanced analysis completed
   Text Analysis: [readability_score, sentiment_score, language_detected, key_phrases, named_entities]
   Structure Analysis: [sections, headers, lists, tables, formatting]
   Semantic Analysis: [topics, themes, relationships, summary, key_insights]
```

### éŒ¯èª¤è™•ç†å’Œæ¢å¾©ç®¡é“ç¯„ä¾‹

```
ğŸ§ª Testing error handling: nonexistent_file.txt
   Status: error_handled
   Error Type: file_not_found
   Recovery Action: Request document resubmission

ğŸ§ª Testing error handling: empty_file.txt
   Status: error_handled
   Error Type: empty_content
   Recovery Action: Skip processing and notify user

ğŸ§ª Testing error handling: valid_document.txt
   Status: processed
   Error Type: none
```

### å¤šæ–‡ä»¶æ‰¹æ¬¡è™•ç†ç¯„ä¾‹

```
ğŸ“š Starting batch document processing...
âœ… Batch processing completed
   Total Documents: 4
   Successful: 3
   Failed: 1
   Success Rate: 75.0%
   Total Size: 45,678 bytes
   Total Words: 12,345
```

## é…ç½®é¸é …

### ç®¡é“é…ç½®

```csharp
var pipelineOptions = new DocumentPipelineOptions
{
    EnableParallelProcessing = true,              // å•Ÿç”¨ä¸¦è¡ŒåŸ·è¡Œ
    MaxConcurrency = Environment.ProcessorCount, // æœ€å¤§ä¸¦è¡Œä»»å‹™æ•¸
    EnableErrorHandling = true,                   // å•Ÿç”¨éŒ¯èª¤è™•ç†
    EnableRecovery = true,                        // å•Ÿç”¨è‡ªå‹•æ¢å¾©
    BatchSize = 100,                              // æ¯æ‰¹æ–‡ä»¶æ•¸
    ProcessingTimeout = TimeSpan.FromMinutes(30), // è™•ç†é€¾æ™‚
    EnableProgressTracking = true,                 // è¿½è¹¤è™•ç†é€²åº¦
    EnableResultCaching = true,                   // å¿«å–åˆ†æçµæœ
    StorageProvider = new FileSystemStorageProvider("./pipeline-results")
};
```

### æ–‡ä»¶è™•ç†é…ç½®

```csharp
var processingOptions = new DocumentProcessingOptions
{
    SupportedFormats = new[] { ".txt", ".md", ".pdf", ".doc", ".docx" },
    MaxFileSize = 100 * 1024 * 1024,             // 100MB æœ€å¤§æª”æ¡ˆå¤§å°
    EnableContentValidation = true,                // é©—è­‰æ–‡ä»¶å…§å®¹
    EnableMetadataExtraction = true,               // æå–æ–‡ä»¶å…ƒè³‡æ–™
    EnableContentAnalysis = true,                  // åŸ·è¡Œå…§å®¹åˆ†æ
    EnableStructureAnalysis = true,                // åˆ†ææ–‡ä»¶çµæ§‹
    EnableSemanticAnalysis = true,                 // åŸ·è¡Œèªæ„åˆ†æ
    AnalysisDepth = AnalysisDepth.Comprehensive,   // åˆ†ææ·±åº¦ç­‰ç´š
    EnableResultPersistence = true                 // ä¿ç•™åˆ†æçµæœ
};
```

## ç–‘é›£æ’è§£

### å¸¸è¦‹å•é¡Œ

#### æ–‡ä»¶æ”å…¥å¤±æ•—
```bash
# å•é¡Œï¼šæ–‡ä»¶ç„¡æ³•æ”å…¥
# è§£æ±ºæ–¹æ¡ˆï¼šæª¢æŸ¥æª”æ¡ˆæ¬Šé™ä¸¦é©—è­‰æª”æ¡ˆæ ¼å¼
EnableContentValidation = true;
SupportedFormats = new[] { ".txt", ".md", ".pdf" };
```

#### ä¸¦è¡Œè™•ç†å•é¡Œ
```bash
# å•é¡Œï¼šä¸¦è¡Œè™•ç†å°è‡´éŒ¯èª¤
# è§£æ±ºæ–¹æ¡ˆï¼šé™ä½ä¸¦è¡Œæ€§ä¸¦å•Ÿç”¨éŒ¯èª¤è™•ç†
MaxConcurrency = 2;
EnableErrorHandling = true;
```

#### è¨˜æ†¶é«”å•é¡Œ
```bash
# å•é¡Œï¼šå¤§å‹æ–‡ä»¶å°è‡´è¨˜æ†¶é«”å•é¡Œ
# è§£æ±ºæ–¹æ¡ˆï¼šå•Ÿç”¨ä¸²æµä¸¦è¨­å®šè¨˜æ†¶é«”é™åˆ¶
MaxFileSize = 50 * 1024 * 1024; // 50MB é™åˆ¶
EnableStreaming = true;
```

### åµéŒ¯æ¨¡å¼

å•Ÿç”¨è©³ç´°è¨˜éŒ„ä»¥é€²è¡Œç–‘é›£æ’è§£ï¼š

```csharp
// å•Ÿç”¨åµéŒ¯è¨˜éŒ„
var logger = LoggerFactory.Create(builder =>
{
    builder.AddConsole();
    builder.SetMinimumLevel(LogLevel.Debug);
}).CreateLogger<DocumentAnalysisPipelineExample>();

// ä½¿ç”¨åµéŒ¯è¨˜éŒ„é…ç½®ç®¡é“
var debugPipeline = new GraphExecutor("DebugPipeline", "Debug document analysis", logger);
debugPipeline.EnableDebugMode = true;
debugPipeline.LogProcessingSteps = true;
debugPipeline.LogErrorDetails = true;
```

## é€²éšæ¨¡å¼

### è‡ªè¨‚æ–‡ä»¶è™•ç†ç¨‹å¼

```csharp
// å¯¦ç¾è‡ªè¨‚æ–‡ä»¶è™•ç†ç¨‹å¼
public class CustomDocumentProcessor : IDocumentProcessor
{
    public async Task<DocumentAnalysisResult> ProcessAsync(DocumentContext context)
    {
        var content = context.Content;
        var metadata = context.Metadata;
        
        // è‡ªè¨‚è™•ç†é‚è¼¯
        var customAnalysis = await PerformCustomAnalysis(content, metadata);
        
        return new DocumentAnalysisResult
        {
            DocumentId = context.DocumentId,
            AnalysisResults = customAnalysis,
            ProcessingTimestamp = DateTime.UtcNow,
            ProcessorVersion = "1.0.0"
        };
    }
    
    private async Task<Dictionary<string, object>> PerformCustomAnalysis(string content, Dictionary<string, object> metadata)
    {
        // å¯¦ç¾è‡ªè¨‚åˆ†æé‚è¼¯
        await Task.Delay(100); // æ¨¡æ“¬è™•ç†
        
        return new Dictionary<string, object>
        {
            ["custom_metric"] = CalculateCustomMetric(content),
            ["domain_specific_analysis"] = PerformDomainAnalysis(content, metadata)
        };
    }
}
```

### ç®¡é“å”èª¿

```csharp
// å”èª¿å¤šå€‹æ–‡ä»¶è™•ç†ç®¡é“
var orchestrator = new DocumentPipelineOrchestrator
{
    PipelineDefinitions = new Dictionary<string, PipelineDefinition>
    {
        ["financial_documents"] = new PipelineDefinition
        {
            EntryCondition = "content_category == 'financial'",
            PipelineGraph = financialPipeline,
            Priority = 1,
            ProcessingRules = new FinancialProcessingRules()
        },
        ["legal_documents"] = new PipelineDefinition
        {
            EntryCondition = "content_category == 'legal'",
            PipelineGraph = legalPipeline,
            Priority = 2,
            ProcessingRules = new LegalProcessingRules()
        }
    },
    DefaultPipeline = generalPipeline,
    EnableLoadBalancing = true,
    EnableFailover = true
};

var selectedPipeline = orchestrator.SelectPipeline(documentContext);
```

### å¯¦æ™‚è™•ç†

```csharp
// å¯¦ç¾å¯¦æ™‚æ–‡ä»¶è™•ç†
var realTimePipeline = new RealTimeDocumentPipeline
{
    InputQueue = new DocumentQueue(),
    ProcessingWorkers = new List<DocumentWorker>(),
    ResultPublisher = new ResultPublisher(),
    EnableStreaming = true,
    ProcessingMode = ProcessingMode.RealTime
};

// å•Ÿå‹•å¯¦æ™‚è™•ç†
await realTimePipeline.StartAsync();

// è¨‚é–±å¯¦æ™‚çµæœ
realTimePipeline.ResultPublished += (sender, e) =>
{
    Console.WriteLine($"ğŸ“Š Real-time result: {e.DocumentId} - {e.AnalysisSummary}");
};
```

## ç›¸é—œç¯„ä¾‹

* [æ¢ä»¶ç¯€é»](./conditional-nodes.md)ï¼šå‹•æ…‹è·¯ç”±å’Œæ±ºç­–
* [ä¸²æµåŸ·è¡Œ](./streaming-execution.md)ï¼šå¯¦æ™‚è™•ç†å’Œç›£æ§
* [å¤šä»£ç†](./multi-agent.md)ï¼šå”èª¿æ–‡ä»¶è™•ç†
* [æª¢æŸ¥é»](./checkpointing.md)ï¼šç‹€æ…‹æŒçºŒæ€§å’Œæ¢å¾©

## å¦è¦‹

* [æ–‡ä»¶è™•ç†æ¦‚å¿µ](../concepts/document-processing.md)ï¼šäº†è§£æ–‡ä»¶åˆ†æ
* [ç®¡é“æ¨¡å¼](../patterns/pipeline.md)ï¼šå»ºç½®è™•ç†ç®¡é“
* [æ•ˆèƒ½æœ€ä½³åŒ–](../how-to/performance-optimization.md)ï¼šå„ªåŒ–è™•ç†æ•ˆèƒ½
* [API åƒè€ƒ](../api/)ï¼šå®Œæ•´çš„ API æ–‡ä»¶
